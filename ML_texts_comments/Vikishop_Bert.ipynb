{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "72b1a5d5",
      "metadata": {
        "id": "72b1a5d5"
      },
      "source": [
        "<div style=\"border:solid green 2px; padding: 20px\">\n",
        "<b>Привет, Алексей!</b>\n",
        "\n",
        "Меня зовут Александр Пономаренко, и я буду проверять твой проект. Предлагаю общаться на «ты» :) Но если это не удобно - дай знать, и мы перейдем на \"вы\". \n",
        "\n",
        "Моя основная цель — не указать на совершенные тобою ошибки, а поделиться своим опытом и помочь тебе стать data science. Ты уже проделал большую работу над проектом, но давай сделаем его еще лучше. Ниже ты найдешь мои комментарии - **пожалуйста, не перемещай, не изменяй и не удаляй их**. Увидев у тебя ошибку, в первый раз я лишь укажу на ее наличие и дам тебе возможность самой найти и исправить ее. На реальной работе твой начальник будет поступать так же, а я пытаюсь подготовить тебя именно к работе аналитиком. Но если ты пока не справишься с такой задачей - при следующей проверке я дам более точную подсказку. Я буду использовать цветовую разметку:\n",
        "\n",
        "<div class=\"alert alert-danger\">\n",
        "<b>Комментарий ревьюера ❌:</b> Так выделены самые важные замечания. Без их отработки проект не будет принят. </div>\n",
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "<b>Комментарий ревьюера ⚠️:</b> Так выделены небольшие замечания. Я надеюсь, что их ты тоже учтешь - твой проект от этого станет только лучше. Но настаивать на их отработке не буду.\n",
        "\n",
        "</div>\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "<b>Комментарий ревьюера ✔️:</b> Так я выделяю все остальные комментарии.</div>\n",
        "\n",
        "Давай работать над проектом в диалоге: **если ты что-то меняешь в проекте или отвечаешь на мои комменатри — пиши об этом.** Мне будет легче отследить изменения, если ты выделишь свои комментарии:\n",
        "<div class=\"alert alert-info\"> <b>Комментарий студента:</b> Например, вот так.</div>\n",
        "\n",
        "Всё это поможет выполнить повторную проверку твоего проекта оперативнее. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b7d7ca2-7732-44b4-8c95-6b1174d89eeb",
      "metadata": {
        "id": "7b7d7ca2-7732-44b4-8c95-6b1174d89eeb"
      },
      "source": [
        "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb083dba-8726-42e1-a0f3-4f0cdb2c02bc",
      "metadata": {
        "id": "cb083dba-8726-42e1-a0f3-4f0cdb2c02bc"
      },
      "source": [
        "# Проект для «Викишоп» с BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41360b91-3ddc-47a3-8019-aadf307b8c4e",
      "metadata": {
        "id": "41360b91-3ddc-47a3-8019-aadf307b8c4e"
      },
      "source": [
        "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
        "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
        "Постройте модель со значением метрики качества F1 не меньше 0.75. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ABMvId_fIRtl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABMvId_fIRtl",
        "outputId": "92f2d477-f6df-4057-b127-42630d3eb4df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jupyter_server\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNZpyprM8MZM",
        "outputId": "c7b1f3a5-ce92-401f-c6b4-917777aa6f7c"
      },
      "id": "DNZpyprM8MZM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: jupyter_server in /usr/local/lib/python3.9/dist-packages (2.5.0)\n",
            "Requirement already satisfied: jupyter-server-terminals in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (0.4.4)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (5.3.0)\n",
            "Requirement already satisfied: tornado>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (6.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (0.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (3.1.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (21.3.0)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (3.6.2)\n",
            "Requirement already satisfied: send2trash in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=7.4.4 in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (8.1.0)\n",
            "Requirement already satisfied: traitlets>=5.6.0 in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (5.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (23.0)\n",
            "Requirement already satisfied: nbformat>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (5.8.0)\n",
            "Requirement already satisfied: jupyter-events>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (0.6.3)\n",
            "Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (6.5.4)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (0.16.0)\n",
            "Requirement already satisfied: pyzmq>=24 in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (25.0.2)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.9/dist-packages (from jupyter_server) (1.5.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/dist-packages (from anyio>=3.1.0->jupyter_server) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio>=3.1.0->jupyter_server) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=7.4.4->jupyter_server) (6.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=7.4.4->jupyter_server) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core!=5.0.*,>=4.12->jupyter_server) (3.2.0)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /usr/local/lib/python3.9/dist-packages (from jupyter-events>=0.4.0->jupyter_server) (2.0.7)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.9/dist-packages (from jupyter-events>=0.4.0->jupyter_server) (6.0)\n",
            "Requirement already satisfied: jsonschema[format-nongpl]>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-events>=0.4.0->jupyter_server) (4.3.3)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from jupyter-events>=0.4.0->jupyter_server) (0.1.1)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.9/dist-packages (from jupyter-events>=0.4.0->jupyter_server) (0.1.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (0.2.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (0.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (0.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (4.11.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (6.0.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (1.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (2.1.2)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (2.14.0)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (0.7.2)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (1.2.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert>=6.4.4->jupyter_server) (4.9.2)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.3.0->jupyter_server) (2.16.3)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.9/dist-packages (from terminado>=0.8.3->jupyter_server) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->jupyter_server) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.3->jupyter-client>=7.4.4->jupyter_server) (3.15.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.4.0->jupyter_server) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.4.0->jupyter_server) (22.2.0)\n",
            "\u001b[33mWARNING: jsonschema 4.3.3 does not provide the extra 'format-nongpl'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=7.4.4->jupyter_server) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter_server) (1.15.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter_server) (2.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert>=6.4.4->jupyter_server) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter_server) (2.21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6AEQQjQWIVS-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AEQQjQWIVS-",
        "outputId": "5be39093-0a95-402c-89b2-08a9e3a16618"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.9/dist-packages (1.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.9/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from catboost) (1.4.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from catboost) (5.13.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from catboost) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->catboost) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (1.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (8.4.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (4.39.3)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (5.12.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly->catboost) (8.2.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c99a169-ad01-4f26-a8b6-913e0679abf7",
      "metadata": {
        "id": "5c99a169-ad01-4f26-a8b6-913e0679abf7"
      },
      "source": [
        "## Импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf692ee9-70ab-40be-8b4f-fe3d7638253e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf692ee9-70ab-40be-8b4f-fe3d7638253e",
        "outputId": "eaaf0cf9-1cf8-462c-f919-c7c97fa46dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from tqdm import notebook\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, DistilBertTokenizer, DistilBertConfig, DistilBertForSequenceClassification\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import requests\n",
        "import os\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import TensorDataset\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, PreTrainedModel\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# загрузка стоп-слов на английском языке\n",
        "nltk.download('stopwords')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# загрузка модуля лемматизации слов из библиотеки nltk\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "from sklearn.pipeline import Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4fef183-5025-4cc8-9f15-7608edb15de8",
      "metadata": {
        "id": "e4fef183-5025-4cc8-9f15-7608edb15de8"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('https://code.s3.yandex.net/datasets/toxic_comments.csv', index_col=[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c5ec95d",
      "metadata": {
        "id": "9c5ec95d"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-warning\">\n",
        "<b>Комментарий ревьюера ⚠️:</b>Да, хорошо, только лучше, чтобы и на платформе ЯП запускался код\n",
        "    \n",
        "    \n",
        "```python\n",
        "path_yandex = 'datasets/toxic_comments.csv'    \n",
        "path = 'toxic_comments.csv'\n",
        "\n",
        "if os.path.exists(path):\n",
        "    data = pd.read_csv(path, index_col=[0])\n",
        "    display(data.head(5))\n",
        "elif os.path.exists(path_yandex):\n",
        "    data = pd.read_csv(path_yandex, index_col=[0])\n",
        "    display(data.head(5))\n",
        "else:\n",
        "    print('Ошибка в считывании данных')   \n",
        "```\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b0ee1e-1ee1-471c-88eb-a3b1e898cafb",
      "metadata": {
        "id": "05b0ee1e-1ee1-471c-88eb-a3b1e898cafb"
      },
      "source": [
        "<div class=\"alert alert-info\"> <b>Комментарий студента:</b> Поправил </div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cc8bfdd-6361-4f1a-a27e-df29eaacb669",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cc8bfdd-6361-4f1a-a27e-df29eaacb669",
        "outputId": "51ed4665-24fc-48d8-9cf4-fc285a345ac8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 159292 entries, 0 to 159450\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   text    159292 non-null  object\n",
            " 1   toxic   159292 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 3.6+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e59b981-c3b9-4bb2-98ce-70e76e935be3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e59b981-c3b9-4bb2-98ce-70e76e935be3",
        "outputId": "6f6eea83-ccb3-4f70-8856-56650e2cd140"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     0.0\n",
              "toxic    0.0\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.isna().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2591aee2-8a4d-4435-bec3-7cf4fa1927e1",
      "metadata": {
        "id": "2591aee2-8a4d-4435-bec3-7cf4fa1927e1"
      },
      "source": [
        "## Подготовим выборки"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4404a8f3-257b-4b92-8fd9-e48d1e8104da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4404a8f3-257b-4b92-8fd9-e48d1e8104da",
        "outputId": "40825e53-aefb-4ecf-98e7-1176d04a5909"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df['toxic'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84bc59d1-7076-4f59-b85f-8af4446acaa5",
      "metadata": {
        "id": "84bc59d1-7076-4f59-b85f-8af4446acaa5"
      },
      "outputs": [],
      "source": [
        "# функция для предобработки текста\n",
        "def lemmatize_text(text):\n",
        "    # токенизация\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    \n",
        "    # удаление стоп-слов и неалфавитных символов\n",
        "    tokens = [t for t in tokens if t not in stopwords and t.isalpha()]\n",
        "    \n",
        "    # лемматизация\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    \n",
        "    # объединение слов в строку и возврат\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "# предобработка текста\n",
        "df['text'] = df['text'].apply(lemmatize_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f45e6b0-9056-43ad-93d5-1fb8f3b44cda",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f45e6b0-9056-43ad-93d5-1fb8f3b44cda",
        "outputId": "420fdab9-757e-4262-ca37-d7c21b564259"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((159292, 1), (159292,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "features = df.drop(columns=['toxic'], axis=1)\n",
        "target = df['toxic']\n",
        "features.shape, target.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d442ba3a-b54f-4b79-97c8-50d31b17dfb8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d442ba3a-b54f-4b79-97c8-50d31b17dfb8",
        "outputId": "937f6bcb-5e64-49f1-e3be-1e9476b6c31d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7999962333324963"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(features['text'], target, test_size = 0.2, random_state=45) \n",
        "X_train.shape[0]/df.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25b81feb-e313-4961-84dc-c6527048c0d9",
      "metadata": {
        "id": "25b81feb-e313-4961-84dc-c6527048c0d9"
      },
      "outputs": [],
      "source": [
        "# Вычисление весов классов\n",
        "def class_weights(y):\n",
        "    class_weights = {}\n",
        "    n_classes = len(np.unique(y))\n",
        "    bincount = np.bincount(y)\n",
        "    n_samples = len(y)\n",
        "    for i in range(n_classes):\n",
        "        class_weights[i] = n_samples / (n_classes * bincount[i])\n",
        "    return print(\"Weights:\", class_weights,\n",
        "                 \"relation: {:.2f}\".format(class_weights[1]/class_weights[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "558a24ee-882f-407e-a412-0fd4ab3f1945",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "558a24ee-882f-407e-a412-0fd4ab3f1945",
        "outputId": "82042962-3915-4936-e977-e6b4133a7784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: {0: 0.5566023725911561, 1: 4.916775985801373} relation: 8.83\n"
          ]
        }
      ],
      "source": [
        "class_weights(y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e88da8e-b32c-4135-95d1-829bbb06ac7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e88da8e-b32c-4135-95d1-829bbb06ac7a",
        "outputId": "6d1e0f99-a605-4efe-d747-abec804e28f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: {0: 0.5563530315730651, 1: 4.936318562132011} relation: 8.87\n"
          ]
        }
      ],
      "source": [
        "class_weights(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce4d757-63f7-4e28-a092-01647553cf7f",
      "metadata": {
        "id": "cce4d757-63f7-4e28-a092-01647553cf7f"
      },
      "source": [
        "Разделили выборки в соотношении 0.8:0.2: X_train:X_test. Вес класса в соотношении 1 к 8.83 для обучающей и 8.87 для тестовой"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eeba971",
      "metadata": {
        "id": "8eeba971"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>Комментарий ревьюера ✔️:</b> Хорошо</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c92abc8e",
      "metadata": {
        "id": "c92abc8e"
      },
      "source": [
        "<div class=\"alert alert-danger\">\n",
        "<b>Комментарий ревьюера ❌:</b> Перед tfidf следует данные обработать: удалить мусор + привести слова в нормальную форму (провести лемматизицаю)\n",
        "    \n",
        "+  https://webdevblog.ru/podhody-lemmatizacii-s-primerami-v-python/\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9034074d-6bf9-47f9-bb81-0fd152d60701",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9034074d-6bf9-47f9-bb81-0fd152d60701",
        "outputId": "6b323d22-5fdd-47d1-e316-cc0b0ffd2e0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    explanation edits made username hardcore metal...\n",
              "1    match background colour seemingly stuck thanks...\n",
              "2    hey man really trying edit war guy constantly ...\n",
              "3    ca make real suggestion improvement wondered s...\n",
              "4                        sir hero chance remember page\n",
              "5               congratulation well use tool well talk\n",
              "6                          cocksucker piss around work\n",
              "7    vandalism matt shirvington article reverted pl...\n",
              "8    sorry word offensive anyway intending write an...\n",
              "9                 alignment subject contrary dulithgow\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "df['text'].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee1509d-5309-4a2e-b539-4214b4b3e2c0",
      "metadata": {
        "id": "5ee1509d-5309-4a2e-b539-4214b4b3e2c0"
      },
      "source": [
        "<div class=\"alert alert-info\"> <b>Комментарий студента:</b> Добавил лемматизицаю перед разделением на выборки </div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f2c9177-b38c-4608-83cc-e171e9d216b4",
      "metadata": {
        "id": "7f2c9177-b38c-4608-83cc-e171e9d216b4"
      },
      "source": [
        "## Catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5da8d91f-fe11-4dd5-b1c2-9c247c942439",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5da8d91f-fe11-4dd5-b1c2-9c247c942439",
        "outputId": "6f7501c6-5e56-4098-aec6-3ed151b8780e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Размер X_train_tfidf: (127433, 129078)\n",
            "Размер X_test_tfidf: (31859, 129078)\n",
            "CPU times: user 5.36 s, sys: 97.7 ms, total: 5.46 s\n",
            "Wall time: 5.53 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "count_tf_idf = TfidfVectorizer()\n",
        "\n",
        "# преобразование TF-IDF для тестовой выборки\n",
        "X_train_tfidf = count_tf_idf.fit_transform(X_train)\n",
        "\n",
        "# преобразование TF-IDF для тестовой выборки\n",
        "X_test_tfidf = count_tf_idf.transform(X_test)\n",
        "\n",
        "# вывод размеров матриц TF-IDF\n",
        "print(\"Размер X_train_tfidf:\", X_train_tfidf.shape)\n",
        "print(\"Размер X_test_tfidf:\", X_test_tfidf.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cbfcb6a",
      "metadata": {
        "id": "8cbfcb6a"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>Комментарий ревьюера ✔️:</b> Отлично, молодец, верно используешь Tfidf.\n",
        "    \n",
        "    \n",
        "Совет: Внутри кросс-валидации происходит разбиение выборки на train и valid. Однако, в таком случае векторизатор обучен на всей выборке(train), а это не совсем корректно. Чтобы избежать это можно воспользоваться Pipeline:\n",
        "    \n",
        "```python\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stopwords)),\n",
        "    ('logreg', LogisticRegression(random_state=42)),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'logreg__C': [1,2,6]\n",
        "}\n",
        "\n",
        "grid_search_tune = RandomizedSearchCV(pipeline, parameters, cv=3, n_jobs=-1, scoring='f1', verbose=3)\n",
        "grid_search_tune.fit(train_features, train_targets)\n",
        "  \n",
        "    \n",
        "```\n",
        "    \n",
        "Это просто каркас, можешь сам выбрать какие параметры использовать для подбора:) \n",
        "    \n",
        "+  https://runebook.dev/ru/docs/scikit_learn/modules/generated/sklearn.model_selection.halvinggridsearchcv - тут про HalvingGridSearchCV\n",
        "    \n",
        "+  https://www.rupython.com/python-sklearn-pipeline-pipeline-28301.html - про pipeline\n",
        "+  https://towardsdatascience.com/how-to-use-sklearn-pipelines-for-ridiculously-neat-code-a61ab66ca90d\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39f40fcc-64d3-45cf-8f61-e2589c9bbdcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39f40fcc-64d3-45cf-8f61-e2589c9bbdcb",
        "outputId": "658c4df1-84f6-4f64-c816-0ba68cb4e0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\tlearn: 0.6777453\ttotal: 251ms\tremaining: 2m 5s\n",
            "100:\tlearn: 0.4459782\ttotal: 14.3s\tremaining: 56.4s\n",
            "200:\tlearn: 0.3975185\ttotal: 26.7s\tremaining: 39.7s\n",
            "300:\tlearn: 0.3723979\ttotal: 38.6s\tremaining: 25.5s\n",
            "400:\tlearn: 0.3525194\ttotal: 49.9s\tremaining: 12.3s\n",
            "499:\tlearn: 0.3370244\ttotal: 1m 3s\tremaining: 0us\n",
            "0:\tlearn: 0.6780828\ttotal: 266ms\tremaining: 2m 12s\n",
            "100:\tlearn: 0.4478262\ttotal: 14.9s\tremaining: 58.8s\n",
            "200:\tlearn: 0.3991734\ttotal: 27.9s\tremaining: 41.5s\n",
            "300:\tlearn: 0.3737235\ttotal: 40.3s\tremaining: 26.6s\n",
            "400:\tlearn: 0.3544239\ttotal: 53.2s\tremaining: 13.1s\n",
            "499:\tlearn: 0.3390798\ttotal: 1m 5s\tremaining: 0us\n",
            "0:\tlearn: 0.6775478\ttotal: 227ms\tremaining: 1m 53s\n",
            "100:\tlearn: 0.4473966\ttotal: 15s\tremaining: 59.4s\n",
            "200:\tlearn: 0.4004797\ttotal: 28.1s\tremaining: 41.8s\n",
            "300:\tlearn: 0.3752910\ttotal: 40.7s\tremaining: 26.9s\n",
            "400:\tlearn: 0.3550584\ttotal: 52.9s\tremaining: 13.1s\n",
            "499:\tlearn: 0.3392314\ttotal: 1m 4s\tremaining: 0us\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('catboost', CatBoostClassifier(iterations=500, random_seed=42, loss_function='Logloss', task_type='GPU', verbose=100))\n",
        "])\n",
        "\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "    'catboost__learning_rate': [0.03, 0.1],\n",
        "    'catboost__depth': [6, 10],\n",
        "    'catboost__l2_leaf_reg': [1, 3],\n",
        "    'catboost__class_weights': [[1, 8.8], [1, 8.9]]\n",
        "}\n",
        "\n",
        "cat_grid_search = GridSearchCV(pipeline, parameters, cv=3, n_jobs=1, scoring='f1')\n",
        "cat_grid_search.fit(X_train, y_train)\n",
        "cat_grid_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TXUE6nbmm9qU",
      "metadata": {
        "id": "TXUE6nbmm9qU"
      },
      "outputs": [],
      "source": [
        "print(f\"Best params: {cat_grid_search.best_params_}\")\n",
        "print(f'RMSE модели на базе библиотеки Catboost {-cat_grid_search.best_score_}')\n",
        "cat_rmse = -(cat_grid_search.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JJdvcuQ8zX0L",
      "metadata": {
        "id": "JJdvcuQ8zX0L"
      },
      "source": [
        "<div class=\"alert alert-info\"><b>Комментарий ревьюера ✔️:</b> Попробовал с pipilin в Colab, правда google после трех прогонов падает, думаю только платная версия будет правильно работать. Правда надо ставить n_jobs = 1. Пока дошел до этого  -- сколько нервов </div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7d699f9-c7af-4c68-8f58-7e0b58e232e3",
      "metadata": {
        "id": "e7d699f9-c7af-4c68-8f58-7e0b58e232e3"
      },
      "outputs": [],
      "source": [
        "#%%time\n",
        "\n",
        "#model = CatBoostClassifier(loss_function='Logloss', eval_metric='F1')\n",
        "\n",
        "#model.fit(X_train_tfidf, y_train, eval_set=(X_test_tfidf, y_test), verbose=100, early_stopping_rounds=100)\n",
        "\n",
        "\n",
        "#y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "\n",
        "#f1 = f1_score(y_test, y_pred)\n",
        "#print(\"F1 Score:\", f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "404cbdae",
      "metadata": {
        "id": "404cbdae"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>Комментарий ревьюера ✔️:</b> Хорошо:)\n",
        "    \n",
        "    \n",
        "P.S что такое 1+1?)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "819ffd51-d5cd-473d-95d7-b6e5dd4ddde2",
      "metadata": {
        "id": "819ffd51-d5cd-473d-95d7-b6e5dd4ddde2"
      },
      "source": [
        "<div class=\"alert alert-info\"><b> Осталось со старого проекта  </div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "603186fb-b12f-493f-b908-f3ea21ad8d24",
      "metadata": {
        "id": "603186fb-b12f-493f-b908-f3ea21ad8d24"
      },
      "source": [
        "## Bert"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a349732b-beca-41d8-98ad-899010294068",
      "metadata": {
        "id": "a349732b-beca-41d8-98ad-899010294068"
      },
      "source": [
        "Стоит отметить, что модель обучена для работы с предложениями до 512 символов. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f25c73-7012-4e8f-9c49-e5a987154a30",
      "metadata": {
        "id": "72f25c73-7012-4e8f-9c49-e5a987154a30"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "1+1\n",
        "# Использование токенизатора и модели `toxic-bert`\n",
        "tokenizer = AutoTokenizer.from_pretrained('unitary/toxic-bert')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('unitary/toxic-bert')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45cea969",
      "metadata": {
        "id": "45cea969"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>Комментарий ревьюера ✔️:</b> Молодец, что взял токсичный берт\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539c4cef-4d0c-450b-827e-378c266ab6fa",
      "metadata": {
        "id": "539c4cef-4d0c-450b-827e-378c266ab6fa"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "1+1\n",
        "# код для подготовки данных\n",
        "max_len = 128\n",
        "\n",
        "tokenized = df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=max_len, truncation=True))\n",
        "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "\n",
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_mask, torch.tensor(df['toxic'].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7f8983f-0162-4661-8994-3923c7f8ba3d",
      "metadata": {
        "id": "e7f8983f-0162-4661-8994-3923c7f8ba3d"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "1+1\n",
        "# Подготовка DataLoader\n",
        "batch_size = 32\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zG0LgpjOlGrA",
      "metadata": {
        "id": "zG0LgpjOlGrA"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "1+1\n",
        "# Выполнение прогнозирования\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.eval()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34f9d1e5-af91-4b72-91eb-203bd5e73fe2",
      "metadata": {
        "id": "34f9d1e5-af91-4b72-91eb-203bd5e73fe2"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "\n",
        "for batch in tqdm(data_loader):\n",
        "    input_ids, attention_mask, _ = [b.to(device) for b in batch]\n",
        "\n",
        "    with torch.no_grad(): \n",
        "        batch_predictions = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    predictions.append(batch_predictions.logits[:, 0].cpu().numpy())\n",
        "\n",
        "features = np.concatenate(predictions, axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb3c7b06-583a-40fe-9a11-f4e0162b9d14",
      "metadata": {
        "id": "fb3c7b06-583a-40fe-9a11-f4e0162b9d14"
      },
      "outputs": [],
      "source": [
        "# Apply threshold\n",
        "threshold = 0.5\n",
        "class_labels = (predictions_array > threshold).astype(int)\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(df['toxic'], class_labels, average='weighted')\n",
        "print(f\"F1 score: {f1}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ead5ce-1769-42a1-a89c-375a35dc0d0b",
      "metadata": {
        "id": "73ead5ce-1769-42a1-a89c-375a35dc0d0b"
      },
      "source": [
        "<div class=\"alert alert-info\"><b> Поменял на графическийб использовал, чуть подправил для сравнения с df['toxic']  </div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e699e276",
      "metadata": {
        "id": "e699e276"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>Комментарий ревьюера ✔️:</b> Дааа, все верно:)\n",
        "\n",
        "Давай воспользуемся `Google Colab` с `GPU`, так получится обработать все данные +- за 5-20 минут \n",
        "    \n",
        "Сначала настроем среду:\n",
        "    \n",
        "+  `Среда выполнения` -> `сменить среду выполнения`    \n",
        "+  Теперь выбираем `GPU` и `сохранить`    \n",
        "    \n",
        "Объявляем `BERT`:\n",
        "    \n",
        "```python\n",
        "tokenizer = BertTokenizer.from_pretrained(\"unitary/toxic-bert\")\n",
        "model = BertModel.from_pretrained(\"unitary/toxic-bert\")    \n",
        "```\n",
        "\n",
        "Далее создаем padded и attention_mask (максимальную длину возьмем 128 этого вполне должно хватить)\n",
        "    \n",
        "```python\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "tokenized = df['text'].progress_apply(lambda x: tokenizer.encode(x, max_length=128, truncation=True, add_special_tokens=True)) #обрежет под нужное кол-во токенов\n",
        "\n",
        "padded = pad_sequence([torch.as_tensor(seq) for seq in tokenized], batch_first=True) #добьет нулями  \n",
        "\n",
        "attention_mask = padded > 0\n",
        "attention_mask = attention_mask.type(torch.LongTensor)    \n",
        "```\n",
        "    \n",
        "Далее нам нужно создать загрузчик и датасет\n",
        "    \n",
        "```python\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "dataset = TensorDataset(attention_mask, padded)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0) # создает загрузчик данных с батчом 32\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\") #проверяет доступно ли нам GPU или нет\n",
        "print(f'Device: {device}')\n",
        "```\n",
        "Про `DataLoader` и `TensorDataset`. Огромный плюс, в том, что тебе не придется самому делать батчи + удобно очень использовать и выгружать данные (на больших проектах очень выручают, поэтому будет круто если разберешься, там нет ничего сложного, если что могу помочь, если не получится) P.S если что, не обращай внимание на collate_fn\n",
        "                \n",
        "+  https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00    \n",
        "    \n",
        " \n",
        "Далее мы, чутка переписываем получения embeddings. Тут мы просто переводим батчи и модель на GPU в процессе выполнения, где и происходят все вычисления\n",
        "    \n",
        "```python\n",
        "embeddings = []\n",
        "model.to(device)\n",
        "model.eval() \n",
        "for attention_mask, padded in notebook.tqdm(dataloader):\n",
        "    attention_mask, padded = attention_mask.to(device), padded.to(device)\n",
        "    \n",
        "    with torch.no_grad(): \n",
        "        batch_embeddings = model(padded, attention_mask=attention_mask)\n",
        "        \n",
        "    embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy()) \n",
        "\n",
        "features = np.concatenate(embeddings)       \n",
        "```\n",
        "    \n",
        "P.S У меня пример для получения embedding, но у тебя будет аналогично:)\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ccc8aa7-6dea-475a-b093-c7738c2547b4",
      "metadata": {
        "id": "6ccc8aa7-6dea-475a-b093-c7738c2547b4"
      },
      "source": [
        "## Вывод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb69a06-5761-4447-915d-9cedbc4a9a39",
      "metadata": {
        "id": "dfb69a06-5761-4447-915d-9cedbc4a9a39"
      },
      "source": [
        "Мы построили инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
        "Обучили модель классифицировать комментарии на позитивные и негативные.\n",
        "Построил модель со значением метрики качества F1 = 0.752(обучалась и предсказывала 30 минут) для catboost и Взяли обученную модель Bert-toxic и построили на ней прогноз и измерили F1= 0.85(предсказывала 5 часов 30 минут)\n",
        "Использовали 2 способа построения определение токсичных коментарий:\n",
        "1. нейроную сеть bert-toxic\n",
        "2. модель машинного обучения catboost\n",
        "У обоих моделей есть плюсы и минусы."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b8ee8a",
      "metadata": {
        "id": "35b8ee8a"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>Комментарий ревьюера ✔️:</b> \n",
        "\n",
        "+  Сделай предобработку для tfidf\n",
        "+  Попробуй прогнать модельку на GPU (если нет времени, то главное проведи предобработку)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3993fb1b",
      "metadata": {
        "id": "3993fb1b"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>Комментарий ревьюера ✔️:</b> \n",
        "\n",
        "Для работы с текстами используют и другие подходы. Например, сейчас активно используются RNN (LSTM) и трансформеры (BERT и другие с улицы Сезам, например, ELMO). НО! Они не являются панацеей, не всегда они нужны, так как и TF-IDF или Word2Vec + модели из классического ML тоже могут справляться. \\\n",
        "BERT тяжелый, существует много его вариаций для разных задач, есть готовые модели, есть надстройки над библиотекой transformers. Если, обучать BERT на GPU (можно в Google Colab или Kaggle), то должно быть побыстрее.\\\n",
        "https://huggingface.co/transformers/model_doc/bert.html \\\n",
        "https://t.me/renat_alimbekov \\\n",
        "https://colah.github.io/posts/2015-08-Understanding-LSTMs/ - Про LSTM \\\n",
        "https://web.stanford.edu/~jurafsky/slp3/10.pdf - про энкодер-декодер модели, этеншены\\\n",
        "https://pytorch.org/tutorials/beginner/transformer_tutorial.html - официальный гайд\n",
        "по трансформеру от создателей pytorch\\\n",
        "https://transformer.huggingface.co/ - поболтать с трансформером \\\n",
        "Библиотеки: allennlp, fairseq, transformers, tensorflow-text — множество реализованных\n",
        "методов для трансформеров методов NLP \\\n",
        "Word2Vec https://radimrehurek.com/gensim/models/word2vec.html \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0d375c7-142f-4b4a-84ca-e8c91d5c5599",
      "metadata": {
        "id": "e0d375c7-142f-4b4a-84ca-e8c91d5c5599"
      },
      "source": [
        "## Чек-лист проверки"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "082537c5-b660-44e4-ac3c-e92f9c62a088",
      "metadata": {
        "id": "082537c5-b660-44e4-ac3c-e92f9c62a088"
      },
      "source": [
        "- [x]  Jupyter Notebook открыт\n",
        "- [x]  Весь код выполняется без ошибок\n",
        "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
        "- [x]  Данные загружены и подготовлены\n",
        "- [x]  Модели обучены\n",
        "- [x]  Значение метрики *F1* не меньше 0.75\n",
        "- [x]  Выводы написаны"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80a32199",
      "metadata": {
        "id": "80a32199"
      },
      "source": [
        "<font color='blue'><b>Итоговый комментарий ревьюера</b></font>\n",
        "<div class=\"alert alert-success\">\n",
        "<b>Комментарий ревьюера ✔️:</b>Алексей, получился хороший проект! \n",
        "    \n",
        "Если есть  если есть какие либо вопросы я с удовольствием на них отвечу:) <br> Исправь, пожалуйста, замечания и жду проект на следующую проверку:) </div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11aeb10d-3ce7-42d8-aaf8-c79d70e6c889",
      "metadata": {
        "id": "11aeb10d-3ce7-42d8-aaf8-c79d70e6c889"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2413732-cdf7-4d24-a811-ee2ab9bc6fd6",
      "metadata": {
        "id": "e2413732-cdf7-4d24-a811-ee2ab9bc6fd6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}